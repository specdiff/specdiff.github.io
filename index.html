<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>SpecDiff & SpecDiff-2</title>
    <meta
      name="description"
      content="Project page for SpecDiff and SpecDiff-2: accelerating LLM decoding with diffusion-model drafters for speculative decoding."
    />
    <meta name="theme-color" content="#0b1020" />
    <meta property="og:title" content="SpecDiff & SpecDiff-2 — Speculative Diffusion Decoding" />
    <meta
      property="og:description"
      content="Accelerate LLM decoding with diffusion-model drafters: draft token blocks in parallel, then verify exactly with the target model."
    />
    <meta property="og:type" content="website" />
    <link rel="icon" href="assets/img/favicon.svg" type="image/svg+xml" />
    <link rel="stylesheet" href="assets/css/style.css" />
  </head>
  <body>
    <header class="topbar">
      <a class="brand" href="#top" aria-label="Go to top">
        <span class="brand__title">SpecDiff</span>
        <span class="brand__subtitle">Speculative diffusion decoding</span>
      </a>

      <button class="nav__toggle" type="button" aria-label="Toggle navigation" aria-expanded="false">
        <span></span><span></span><span></span>
      </button>

      <nav class="nav" aria-label="Primary">
        <a href="#overview">Overview</a>
        <a href="#papers">Papers</a>
        <a href="#results">Results</a>
        <a href="#blog">Blog</a>
        <a href="#bibtex">BibTeX</a>
      </nav>
    </header>

    <main id="top">
      <section class="hero">
        <div class="container hero__grid">
          <div>
            <p class="pill">Project page</p>
            <h1>SpecDiff &amp; SpecDiff-2</h1>
            <p class="lede">
              Accelerate large language model decoding with <strong>diffusion-model drafters</strong>:
              draft many tokens in parallel, then verify exactly with the target LLM.
            </p>

            <p class="authors">
              <strong>SpecDiff:</strong> Jacob K. Christopher · Brian R. Bartoldson · Tal Ben-Nun · Michael Cardei · Bhavya Kailkhura · Ferdinando Fioretto<br />
              <strong>SpecDiff-2:</strong> Jameson Sandler · Jacob K. Christopher · Thomas Hartvigsen · Ferdinando Fioretto
            </p>

            <div class="meta">
              <div class="meta__row">
                <span class="meta__k">SpecDiff</span>
                <span class="meta__v">arXiv:2408.05636</span>
              </div>
              <div class="meta__row">
                <span class="meta__k">SpecDiff-2</span>
                <span class="meta__v">arXiv:2511.00606</span>
              </div>
            </div>

            <div class="cta">
              <a class="btn btn--primary" href="https://arxiv.org/pdf/2511.00606" target="_blank" rel="noreferrer">
                SpecDiff-2 PDF
              </a>
              <a class="btn" href="https://arxiv.org/pdf/2408.05636" target="_blank" rel="noreferrer">SpecDiff PDF</a>
              <a class="btn btn--ghost" href="#blog">Read the blog</a>
            </div>

            <p class="note">
              This page is intended to be a readable, practitioner-friendly walkthrough. For the authoritative
              details (derivations, hyperparameters, and full experiments), see the papers.
            </p>
          </div>

          <figure class="hero__figure" aria-label="Teaser diagram">
            <img src="assets/img/key_result.png" alt="Key result figure comparing relative speed-up across methods and model sizes." />
          </figure>
        </div>
      </section>

      <section id="overview" class="section">
        <div class="container">
          <h2>Overview</h2>
          <div class="two-col">
            <div class="card">
              <h3>What is speculative diffusion decoding?</h3>
              <p>
                Speculative decoding is a <em>draft-then-verify</em> framework: a fast <strong>drafter</strong> proposes a
                short continuation, and a large <strong>verifier</strong> checks those tokens in parallel. If the draft is
                likely under the verifier, you accept a long prefix in one shot.
              </p>
              <p>
                SpecDiff replaces the autoregressive drafter with a <strong>masked discrete diffusion model</strong>
                that drafts an entire window of tokens <em>in parallel</em> via iterative denoising. This removes a major
                bottleneck: drafting no longer requires token-by-token generation.
              </p>
            </div>

            <div class="card">
              <h3>What does SpecDiff-2 add?</h3>
              <p>
                SpecDiff-2 focuses on a core practical issue: <strong>drafter–verifier alignment</strong>. Diffusion
                drafters can be extremely parallel, but if they propose tokens the verifier often rejects, speed-up
                evaporates.
              </p>
              <ul class="bullets">
                <li><strong>Train-time alignment:</strong> streak-distillation to optimize for long accepted prefixes.</li>
                <li><strong>Test-time alignment:</strong> self-selection over multiple parallel drafts to maximize expected throughput.</li>
                <li><strong>Scaling insight:</strong> “acceleration–compute” scaling links faster decoding to better performance under fixed time budgets.</li>
              </ul>
            </div>
          </div>

          <div class="figure-grid">
            <figure class="figure figure--tight">
              <img src="assets/img/train_time_fig.png" alt="Train-time acceleration diagram: streak-distillation aligns diffusion drafter with verifier." />
              <figcaption>Train-time acceleration: streak-distillation trains the diffusion drafter to produce long accepted streaks.</figcaption>
            </figure>
            <figure class="figure figure--tight">
              <img src="assets/img/test_time_fig.png" alt="Test-time acceleration diagram: self-selection chooses the best among multiple diffusion drafts." />
              <figcaption>Test-time acceleration: self-selection picks the draft expected to yield highest throughput.</figcaption>
            </figure>
          </div>

          <!-- <figure class="figure">
            <img src="assets/img/streak-distill.png" alt="Position-wise acceptance rate vs distance from prefix for base, AR-distillation, and streak-distillation." />
            <figcaption>Streak-distillation maintains higher acceptance deeper into the draft window (reported +3.2× at later positions).</figcaption>
          </figure> -->
        </div>
      </section>

      <section id="papers" class="section section--alt">
        <div class="container">
          <h2>Papers</h2>
          <div class="paper-grid">
            <article class="paper">
              <div class="paper__tag">SpecDiff</div>
              <h3>Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion</h3>
              <p class="paper__links">
                <a href="https://arxiv.org/pdf/2408.05636" target="_blank" rel="noreferrer">PDF</a>
                <span class="sep">·</span>
                <a href="https://arxiv.org/abs/2408.05636" target="_blank" rel="noreferrer">arXiv</a>
              </p>
              <p>
                Introduces diffusion-model drafters for speculative decoding. Shows that masked discrete diffusion
                can draft long windows efficiently, enabling parallelism in both drafting and verification.
              </p>
            </article>

            <article class="paper paper--featured">
              <div class="paper__tag">SpecDiff-2</div>
              <h3>SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding</h3>
              <p class="paper__links">
                <a href="https://arxiv.org/pdf/2511.00606" target="_blank" rel="noreferrer">PDF</a>
                <span class="sep">·</span>
                <a href="https://arxiv.org/abs/2511.00606" target="_blank" rel="noreferrer">arXiv</a>
              </p>
              <p>
                Adds principled alignment mechanisms tailored to diffusion drafters (streak-distillation + self-selection),
                producing stronger end-to-end throughput while preserving lossless verification.
              </p>
            </article>
          </div>
        </div>
      </section>

      <section id="results" class="section">
        <div class="container">
          <h2>Headline Results (from the papers)</h2>
          <div class="callout">
            <p>
              Numbers depend on model/task/settings. The goal here is to capture the “shape” of the gains; please
              cite the papers for exact tables and experimental details.
            </p>
          </div>

          <div class="table-wrap" role="region" aria-label="Results table" tabindex="0">
            <table>
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Key idea</th>
                  <th>Reported speed/throughput highlights</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>SpecDiff</strong></td>
                  <td>Masked discrete diffusion drafter</td>
                  <td>Up to <strong>7.2×</strong> vs vanilla decoding; up to <strong>1.75×</strong> vs prior speculative baselines (reported).</td>
                </tr>
                <tr>
                  <td><strong>SpecDiff-2</strong></td>
                  <td>Alignment + multi-draft self-selection</td>
                  <td>
                    Average <strong>4.22×</strong> speed-ups; up to <strong>5.5×</strong>; <strong>+55%</strong> throughput vs prior baselines (reported).
                  </td>
                </tr>
                <tr>
                  <td><strong>SpecDiff-2 (time budget)</strong></td>
                  <td>Acceleration–compute scaling</td>
                  <td>On a 15s math reasoning budget: <strong>+63%</strong> accuracy vs vanilla; <strong>+11%</strong> vs unaligned diffusion drafting (reported).</td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- <figure class="figure" style="margin-top: 16px">
            <img src="assets/img/cot-reasoning-scaling.png" alt="Acceleration–compute scaling: higher accuracy under fixed reasoning budgets with faster decoding." />
            <figcaption>Acceleration–compute scaling: faster decoding translates into higher accuracy under fixed time budgets.</figcaption>
          </figure> -->
        </div>
      </section>

      <section id="blog" class="section section--alt">
        <div class="container">
          <h2>Blog</h2>
          <p class="lede">
            Two-part walkthrough: first the core SpecDiff idea, then SpecDiff-2’s alignment improvements and the
            strongest results.
          </p>

          <div class="blog">
            <article class="blog__post">
              <header class="blog__header">
                <p class="blog__eyebrow">Part I</p>
                <h3>SpecDiff: diffusion as a speculative drafter</h3>
              </header>

              <div class="prose">
                <p>
                  If you’ve used speculative decoding before, the story is familiar: a small model drafts tokens and the
                  large model verifies them in parallel. The speed-up you get is proportional to how many draft tokens
                  the verifier accepts per iteration.
                </p>
                <p>
                  The catch is that most speculative systems still draft <em>autoregressively</em>, which means the drafter
                  itself is sequential. SpecDiff asks: what if the drafter could propose a whole block of tokens at once?
                </p>

                <div class="pullquote">
                  <strong>Key idea:</strong> Use a masked discrete diffusion language model as the drafter so drafting a window of
                  <span class="inline-math">γ</span> tokens is parallel over positions (with cost controlled by diffusion steps).
                </div>

                <p>
                  Concretely, the diffusion drafter starts from a masked window and iteratively denoises to produce
                  token distributions for every position. The verifier then scores those positions in parallel and accepts the
                  longest prefix that passes the acceptance rule. The loop repeats until EOS.
                </p>

                <p class="aside">
                  In practice, SpecDiff highlights an important theme for diffusion drafters: <em>alignment</em> matters as much as raw parallelism.
                </p>
              </div>
            </article>

            <article class="blog__post blog__post--featured">
              <header class="blog__header">
                <p class="blog__eyebrow">Part II</p>
                <h3>SpecDiff-2: scaling alignment (and scaling wins)</h3>
              </header>

              <div class="prose">
                <p>
                  SpecDiff-2 treats the main failure mode head-on: even a very fast diffusion drafter is not useful if the verifier
                  rejects most of its proposed tokens. When acceptance is low—especially later in the draft window—you end up paying to
                  draft tokens that never get committed.
                </p>

                <div class="pullquote">
                  <strong>SpecDiff-2 in one line:</strong> keep diffusion drafting, but align the drafter to maximize the length of accepted
                  <em>streaks</em> (train-time), and pick the best among multiple candidate drafts (test-time).
                </div>

                <h4>Train-time acceleration: streak-distillation</h4>
                <p>
                  Instead of optimizing for token-wise similarity, streak-distillation optimizes for long contiguous accepted prefixes.
                  The intuition is simple: the verifier commits a prefix, not independent tokens, so the objective should directly reward
                  long accepted runs.
                </p>

                <figure class="figure">
                  <img src="assets/img/streak-distill_speedup.png" alt="Math500 speed-up vs streak-distillation steps, showing improvements for Qwen2.5-14B and Qwen2.5-72B." />
                  <figcaption>Streak-distillation training improves Math500 speed-up over time (reported).</figcaption>
                </figure>

                <h4>Test-time acceleration: self-selection</h4>
                <p>
                  At inference time, diffusion models make it cheap to sample multiple candidate drafts in parallel. SpecDiff-2 uses this to
                  generate <span class="inline-math">K</span> drafts, estimate expected throughput, and verify only the best draft.
                </p>

                <figure class="figure figure--tight">
                  <img src="assets/img/test_time_fig.png" alt="Test-time acceleration diagram illustrating self-selection among K drafts." />
                  <figcaption>Self-selection: generate multiple drafts and verify the one expected to yield the best throughput.</figcaption>
                </figure>

                <h4>Acceleration–compute scaling</h4>
                <p>
                  One of the most compelling takeaways is that faster decoding can translate into better results when users (or systems)
                  operate under time constraints. If you can generate more tokens in the same wall-clock budget, you can allocate more “thinking”
                  to reasoning-heavy tasks.
                </p>

                <figure class="figure">
                  <img src="assets/img/cot-reasoning-scaling.png" alt="Math500 accuracy improves with increased reasoning budget; SpecDiff-2 reaches higher accuracy at fixed budget." />
                  <figcaption>Faster decoding → higher accuracy under fixed budgets (reported on Math500 with a 15s budget).</figcaption>
                </figure>
              </div>
            </article>
          </div>
        </div>
      </section>

      <section id="bibtex" class="section">
        <div class="container">
          <h2>BibTeX</h2>
          <div class="two-col">
            <div class="card">
              <div class="card__top">
                <h3>SpecDiff</h3>
                <button class="btn btn--small js-copy" type="button" data-copy-target="#bib-specdiff">
                  Copy
                </button>
              </div>
              <pre id="bib-specdiff" class="bib">@article{specdiff2024,
  title   = {Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion},
  author  = {Christopher, Jacob K. and Bartoldson, Brian R. and Ben-Nun, Tal and Cardei, Michael and Kailkhura, Bhavya and Fioretto, Ferdinando},
  journal = {arXiv preprint arXiv:2408.05636},
  year    = {2024}
}</pre>
            </div>

            <div class="card">
              <div class="card__top">
                <h3>SpecDiff-2</h3>
                <button class="btn btn--small js-copy" type="button" data-copy-target="#bib-specdiff2">
                  Copy
                </button>
              </div>
              <pre id="bib-specdiff2" class="bib">@article{specdiff22025,
  title   = {SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding},
  author  = {Sandler, Jameson and Christopher, Jacob K. and Hartvigsen, Thomas and Fioretto, Ferdinando},
  journal = {arXiv preprint arXiv:2511.00606},
  year    = {2025}
}</pre>
            </div>
          </div>
        </div>
      </section>
    </main>

    <footer class="footer">
      <div class="container footer__grid">
        <div>
          <div class="brand brand--footer">
            <span class="brand__title">SpecDiff</span>
            <span class="brand__subtitle">Project page</span>
          </div>
          <p class="footer__note">
            Built as a lightweight static site for GitHub Pages. Replace the placeholders with your preferred
            affiliations, additional links (code/demo), and figures from the papers.
          </p>
        </div>
        <div class="footer__links" aria-label="Footer links">
          <a href="#top">Top</a>
          <a href="#papers">Papers</a>
          <a href="#blog">Blog</a>
          <a href="#bibtex">BibTeX</a>
        </div>
      </div>
    </footer>

    <script src="assets/js/main.js"></script>
  </body>
</html>
