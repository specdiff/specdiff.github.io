<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Speculative Diffusion (SpecDiff)</title>
    <meta
      name="description"
      content="Project page for SpecDiff and SpecDiff-2: accelerating LLM decoding with diffusion-model drafters for speculative decoding."
    />
    <meta name="theme-color" content="#0b1020" />
    <meta property="og:title" content="Speculative Diffusion (SpecDiff)" />
    <meta
      property="og:description"
      content="Accelerate LLM decoding with diffusion-model drafters: draft token blocks in parallel, then verify exactly with the target model."
    />
    <meta property="og:type" content="website" />
    <link rel="icon" href="assets/img/favicon.svg" type="image/svg+xml" />
    <link rel="stylesheet" href="assets/css/style.css" />
  </head>
  <body>
    <header class="topbar">
      <a class="brand" href="#top" aria-label="Go to top">
        <span class="brand__title">SpecDiff</span>
        <span class="brand__subtitle">Speculative diffusion decoding</span>
      </a>

      <button class="nav__toggle" type="button" aria-label="Toggle navigation" aria-expanded="false">
        <span></span><span></span><span></span>
      </button>

      <nav class="nav" aria-label="Primary">
        <a href="#overview">Overview</a>
        <a href="#papers">Papers</a>
        <a href="#results">Results</a>
        <a href="#blog">Blog</a>
        <a href="#bibtex">BibTeX</a>
      </nav>
    </header>

    <main id="top">
      <section class="hero">
        <div class="container hero__grid">
          <div>
            <p class="pill">Project page</p>
            <h1>SpecDiff &amp; SpecDiff-2</h1>
            <p class="lede">
              Accelerate large language model decoding with <strong>diffusion-model drafters</strong>:
              draft many tokens in parallel, then verify exactly with the target LLM.
            </p>

            <p class="authors">
              <strong>SpecDiff:</strong> Jacob K. Christopher · Brian R. Bartoldson · Tal Ben-Nun · Michael Cardei · Bhavya Kailkhura · Ferdinando Fioretto<br />
              <strong>SpecDiff-2:</strong> Jameson Sandler · Jacob K. Christopher · Thomas Hartvigsen · Ferdinando Fioretto
            </p>

            <div class="meta">
              <div class="meta__row">
                <span class="meta__k">SpecDiff</span>
                <span class="meta__v">arXiv:2408.05636</span>
              </div>
              <div class="meta__row">
                <span class="meta__k">SpecDiff-2</span>
                <span class="meta__v">arXiv:2511.00606</span>
              </div>
            </div>

            <div class="cta">
              <a class="btn btn--primary" href="https://arxiv.org/pdf/2511.00606" target="_blank" rel="noreferrer">
                SpecDiff-2 PDF
              </a>
              <a class="btn" href="https://arxiv.org/pdf/2408.05636" target="_blank" rel="noreferrer">SpecDiff PDF</a>
              <a class="btn btn--ghost" href="#blog">Read the blog</a>
            </div>

            <p class="note">
              This page is intended to be a readable, practitioner-friendly walkthrough. For the authoritative
              details (derivations, hyperparameters, and full experiments), see the papers.
            </p>
          </div>

          <figure class="hero__figure" aria-label="Teaser diagram">
            <img src="assets/img/key_result.png" alt="Key result figure comparing relative speed-up across methods and model sizes." />
          </figure>
        </div>
      </section>

      <section id="overview" class="section">
        <div class="container">
          <h2>Overview</h2>
          <div class="two-col">
            <div class="card">
              <h3>What is speculative diffusion decoding?</h3>
              <p>
                Speculative decoding is a <em>draft-then-verify</em> framework: a fast <strong>drafter</strong> proposes a
                short continuation, and a large <strong>verifier</strong> checks those tokens in parallel. If the draft is
                likely under the verifier, you accept a long prefix in one shot.
              </p>
              <p>
                SpecDiff replaces the autoregressive drafter with a <strong>masked discrete diffusion model</strong>
                that drafts an entire window of tokens <em>in parallel</em> via iterative denoising. This removes a major
                bottleneck: drafting no longer requires token-by-token generation.
              </p>
            </div>

            <div class="card">
              <h3>What does SpecDiff-2 add?</h3>
              <p>
                SpecDiff-2 focuses on a core practical issue: <strong>drafter–verifier alignment</strong>. Diffusion
                drafters can be extremely parallel, but if they propose tokens the verifier often rejects, speed-up
                evaporates.
              </p>
              <ul class="bullets">
                <li><strong>Train-time alignment:</strong> streak-distillation to optimize for long accepted prefixes.</li>
                <li><strong>Test-time alignment:</strong> self-selection over multiple parallel drafts to maximize expected throughput.</li>
                <li><strong>Scaling insight:</strong> “acceleration–compute” scaling links faster decoding to better performance under fixed time budgets.</li>
              </ul>
            </div>
          </div>

          <p class="sublede">
            <h3>SpecDiff-2 adds two complementary mechanisms on top of diffusion drafting: train-time alignment (streak-distillation) and
            test-time selection (self-selection over parallel drafts).</h3>
          </p>

          <div class="figure-grid">
            <figure class="figure figure--tight">
              <img src="assets/img/train_time_fig.png" alt="Train-time acceleration diagram: streak-distillation aligns diffusion drafter with verifier." />
              <figcaption><strong>SpecDiff-2 (train-time):</strong> streak-distillation trains the diffusion drafter to produce long accepted streaks.</figcaption>
            </figure>
            <figure class="figure figure--tight">
              <img src="assets/img/test_time_fig.png" alt="Test-time acceleration diagram: self-selection chooses the best among multiple diffusion drafts." />
              <figcaption><strong>SpecDiff-2 (test-time):</strong> self-selection picks the draft expected to yield highest throughput.</figcaption>
            </figure>
          </div>

          <!-- <figure class="figure">
            <img src="assets/img/streak-distill.png" alt="Position-wise acceptance rate vs distance from prefix for base, AR-distillation, and streak-distillation." />
            <figcaption>Streak-distillation maintains higher acceptance deeper into the draft window (reported +3.2× at later positions).</figcaption>
          </figure> -->
        </div>
      </section>

      <section id="papers" class="section section--alt">
        <div class="container">
          <h2>Papers</h2>
          <div class="paper-grid">
            <article class="paper">
              <div class="paper__tag">SpecDiff</div>
              <h3>Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion</h3>
              <p class="paper__links">
                <a href="https://arxiv.org/pdf/2408.05636" target="_blank" rel="noreferrer">PDF</a>
                <span class="sep">·</span>
                <a href="https://arxiv.org/abs/2408.05636" target="_blank" rel="noreferrer">arXiv</a>
              </p>
              <p>
                Introduces diffusion-model drafters for speculative decoding. Shows that masked discrete diffusion
                can draft long windows efficiently, enabling parallelism in both drafting and verification.
              </p>
            </article>

            <article class="paper paper--featured">
              <div class="paper__tag">SpecDiff-2</div>
              <h3>SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding</h3>
              <p class="paper__links">
                <a href="https://arxiv.org/pdf/2511.00606" target="_blank" rel="noreferrer">PDF</a>
                <span class="sep">·</span>
                <a href="https://arxiv.org/abs/2511.00606" target="_blank" rel="noreferrer">arXiv</a>
              </p>
              <p>
                Adds principled alignment mechanisms tailored to diffusion drafters (streak-distillation + self-selection),
                producing stronger end-to-end throughput while preserving lossless verification.
              </p>
            </article>
          </div>
        </div>
      </section>

      <section id="results" class="section">
        <div class="container">
          <h2>Headline Results (from the papers)</h2>
          <div class="callout">
            <p>
              Numbers depend on model/task/settings. The goal here is to capture the “shape” of the gains; please
              cite the papers for exact tables and experimental details.
            </p>
          </div>

          <div class="table-wrap" role="region" aria-label="Results table" tabindex="0">
            <table>
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Key idea</th>
                  <th>Reported speed/throughput highlights</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>SpecDiff</strong></td>
                  <td>Masked discrete diffusion drafter</td>
                  <td>Up to <strong>7.2×</strong> vs vanilla decoding; up to <strong>1.75×</strong> vs prior speculative baselines (reported).</td>
                </tr>
                <tr>
                  <td><strong>SpecDiff-2</strong></td>
                  <td>Alignment + multi-draft self-selection</td>
                  <td>
                    Average <strong>4.22×</strong> speed-ups; up to <strong>5.5×</strong>; <strong>+55%</strong> throughput vs prior baselines (reported).
                  </td>
                </tr>
                <tr>
                  <td><strong>SpecDiff-2 (time budget)</strong></td>
                  <td>Acceleration–compute scaling</td>
                  <td>On a 15s math reasoning budget: <strong>+63%</strong> accuracy vs vanilla; <strong>+11%</strong> vs unaligned diffusion drafting (reported).</td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- <figure class="figure" style="margin-top: 16px">
            <img src="assets/img/cot-reasoning-scaling.png" alt="Acceleration–compute scaling: higher accuracy under fixed reasoning budgets with faster decoding." />
            <figcaption>Acceleration–compute scaling: faster decoding translates into higher accuracy under fixed time budgets.</figcaption>
          </figure> -->
        </div>
      </section>

      <section id="blog" class="section section--alt">
        <div class="container">
          <h2>Blog</h2>
          <!-- <p class="lede">
            Two-part walkthrough: first the core SpecDiff idea, then SpecDiff-2’s alignment improvements and the
            strongest results.
          </p> -->

          <figure class="figure">
            <img
              src="assets/img/comparison-to-sps.png"
              alt="Comparison of classic speculative decoding (sequential drafting bottleneck) versus speculative diffusion decoding (parallel diffusion drafting and improved alignment)."
            />
            <figcaption>Motivation: diffusion drafting removes the sequential drafting bottleneck.</figcaption>
          </figure>

          <div class="blog">
            <article class="blog__post">
              <header class="blog__header">
                <p class="blog__eyebrow">Part I</p>
                <h3>SpecDiff: diffusion as a speculative drafter</h3>
              </header>

              <div class="prose">
                <figure class="figure">
                  <img src="assets/img/specdiff-1-overview.png" alt="High-level overview of SpecDiff: diffusion drafting plus parallel verification and headline speedups." />
                  <figcaption>SpecDiff overview: parallel diffusion drafting + parallel verification.</figcaption>
                </figure>

                <p>
                  Speculative decoding is a <em>draft-then-verify</em> trick: a cheap drafter proposes multiple next tokens,
                  and the target model verifies them in parallel. When the draft matches what the verifier would have done,
                  you “commit” a long prefix in one shot and skip a lot of expensive target-model work.
                </p>
                <p>
                  The catch is that many speculative systems still use an <em>autoregressive</em> drafter. Even if verification is
                  parallel, drafting remains sequential — and that sequential drafting becomes the bottleneck as you scale draft length.
                  SpecDiff’s central move is to replace that sequential drafter with a <strong>masked discrete diffusion model</strong>
                  that drafts an entire window in parallel.
                </p>

                <div class="pullquote">
                  <strong>Key idea:</strong> Use a masked discrete diffusion language model as the drafter so drafting a window of
                  <span class="inline-math">γ</span> tokens is parallel over positions, with compute controlled primarily by the number of diffusion steps.
                </div>

                <h4>What “diffusion drafting” looks like</h4>
                <p>
                  A masked diffusion LM starts from a window of masked tokens and iteratively denoises them. Each denoising
                  step produces token distributions for <em>all positions</em> at once, so the draft window is generated “in parallel”
                  across positions rather than token-by-token.
                </p>

                <p>
                  You can think of it as a text analogue of diffusion for images: the forward process corrupts a clean sequence by masking tokens,
                  and the reverse process learns to reconstruct the original sequence. At inference, you start from an all-masked window and run a
                  small number of denoising steps to propose a complete candidate continuation.
                </p>

                <figure class="figure figure--tight">
                  <img src="assets/img/pipeline.svg" alt="SpecDiff loop: diffusion drafter proposes a token window, verifier scores and accepts a prefix, repeat." />
                  <figcaption>High-level SpecDiff loop: draft a window, verify in parallel, accept a prefix, repeat.</figcaption>
                </figure>

                <h4>Algorithm sketch (high-level)</h4>
                <p>
                  Each iteration looks like: (1) generate a <span class="inline-math">γ</span>-token draft window with the diffusion drafter
                  (in <span class="inline-math">T</span> denoising steps), (2) have the target model compute the corresponding probabilities
                  in parallel, and (3) accept the longest prefix that satisfies the acceptance rule, then continue from the new context.
                </p>

                <p>
                  The important property is that the verifier still defines the distribution: acceptance/rejection is designed so the final output
                  matches the target model’s decoding distribution, while reducing the number of verifier steps needed to produce the same number of tokens.
                </p>

                <h4>Why <span class="inline-math">γ</span> can be larger for diffusion drafters</h4>
                <p>
                  Because drafting is parallel over positions, the cost of increasing the draft length is much smaller than it is for
                  autoregressive drafters. In practice, SpecDiff’s performance becomes more sensitive to the number of denoising steps than
                  to <span class="inline-math">γ</span> itself, which opens the door to longer windows without paying a proportional cost.
                </p>

                <p class="aside">
                  A recurring theme (and the focus of SpecDiff-2): speed-ups come from both <em>parallelism</em> and <em>alignment</em>. If later draft
                  tokens are often rejected, effective throughput drops.
                </p>
              </div>
            </article>

            <article class="blog__post blog__post--featured">
              <header class="blog__header">
                <p class="blog__eyebrow">Part II</p>
                <h3>SpecDiff-2: scaling alignment (and scaling wins)</h3>
              </header>

              <div class="prose">
                <figure class="figure">
                  <img src="assets/img/specdiff-2-overview.png" alt="High-level overview of SpecDiff-2: two-pronged alignment (streak-distillation and self-selection) and headline results." />
                  <figcaption>SpecDiff-2 overview: train-time streak-distillation + test-time self-selection.</figcaption>
                </figure>

                <p>
                  SpecDiff-2 treats the core practical issue head-on: <strong>drafter–verifier alignment</strong>.
                  Even if diffusion drafting is highly parallel, the system only speeds up when the verifier accepts long prefixes.
                  When acceptance is low — especially later in the draft window — you spend compute on tokens that never get committed.
                </p>

                <div class="pullquote">
                  <strong>SpecDiff-2 in one line:</strong> keep diffusion drafting, but align the drafter to maximize the length of accepted
                  <em>streaks</em> (train-time), and pick the best among multiple candidate drafts (test-time).
                </div>

                <h4>Two bottlenecks SpecDiff-2 targets</h4>
                <p>
                  First, classic speculative decoding often uses an autoregressive drafter, which introduces sequential dependency during drafting.
                  Second, even with a non-autoregressive drafter, <em>misalignment</em> between drafter and verifier causes frequent rejections,
                  collapsing the realized speed-up.
                </p>

                <h4>Train-time acceleration: streak-distillation</h4>
                <p>
                  Rather than optimizing token-wise match, streak-distillation optimizes for <em>contiguous accepted prefixes</em>.
                  The intuition is simple: verification commits a prefix, not independent tokens, so the objective should directly reward
                  long accepted runs (“streaks”).
                </p>

                <figure class="figure">
                  <img src="assets/img/streak-distill_speedup.png" alt="Math500 speed-up vs streak-distillation steps, showing improvements for Qwen2.5-14B and Qwen2.5-72B." />
                  <figcaption>Streak-distillation training improves Math500 speed-up over time (reported).</figcaption>
                </figure>

                <!-- <figure class="figure">
                  <img src="assets/img/streak-distill.png" alt="Position-wise acceptance rate vs distance from prefix for base, AR-distillation, and streak-distillation." />
                  <figcaption>Streak-distillation maintains higher acceptance deeper into the draft window (reported +3.2× at later positions).</figcaption>
                </figure> -->

                <h4>Test-time acceleration: self-selection</h4>
                <p>
                  At inference time, diffusion models make it cheap to sample multiple candidate drafts in parallel.
                  SpecDiff-2 uses this to generate <span class="inline-math">K</span> drafts, estimate their expected throughput under the verifier,
                  and verify only the best candidate.
                </p>

                <p>
                  Intuitively, you want the draft that is most likely to yield a long accepted prefix. Self-selection turns that into a lightweight
                  search over parallel candidates — leveraging the fact that diffusion drafters can produce many joint samples from essentially the same
                  underlying position-wise predictions.
                </p>

                <figure class="figure figure--tight">
                  <img src="assets/img/ssa_scaling_larger.png" alt="Test-time acceleration diagram illustrating self-selection among K drafts." />
                  <figcaption>Self-selection: generate multiple drafts and verify the one expected to yield the best throughput.</figcaption>
                </figure>

                <h4>Acceleration–compute scaling</h4>
                <p>
                  A key takeaway is that faster decoding can translate into better results under time constraints.
                  If you can generate more tokens in the same wall-clock budget, you can allocate more “thinking” to reasoning-heavy tasks
                  (e.g., longer reasoning traces or more attempts).
                </p>

                <figure class="figure">
                  <img src="assets/img/cot-reasoning-scaling.png" alt="Math500 accuracy improves with increased reasoning budget; SpecDiff-2 reaches higher accuracy at fixed budget." />
                  <figcaption>Faster decoding → higher accuracy under fixed budgets (reported on Math500 with a 15s budget).</figcaption>
                </figure>
              </div>
            </article>
          </div>
        </div>
      </section>

      <section id="bibtex" class="section">
        <div class="container">
          <h2>BibTeX</h2>
          <div class="stack">
            <div class="card">
              <div class="card__top">
                <h3>SpecDiff</h3>
                <button class="btn btn--small js-copy" type="button" data-copy-target="#bib-specdiff">
                  Copy
                </button>
              </div>
              <pre id="bib-specdiff" class="bib">@inproceedings{christopher2025specdiff,
  title     = {Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion},
  author    = {Christopher, Jacob K. and Bartoldson, Brian R. and Ben-Nun, Tal and Cardei, Michael and Kailkhura, Bhavya and Fioretto, Ferdinando},
  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2025},
  url       = {https://aclanthology.org/2025.naacl-long.601/}
}</pre>
            </div>

            <div class="card">
              <div class="card__top">
                <h3>SpecDiff-2</h3>
                <button class="btn btn--small js-copy" type="button" data-copy-target="#bib-specdiff2">
                  Copy
                </button>
              </div>
              <pre id="bib-specdiff2" class="bib">@inproceedings{sandler2026specdiff2,
  title     = {SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding},
  author    = {Sandler, Jameson and Christopher, Jacob K. and Hartvigsen, Thomas and Fioretto, Ferdinando},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2026},
  url       = {https://mlsys.org/},
  note      = {Accepted to MLSys; arXiv:2511.00606}
}</pre>
            </div>
          </div>
        </div>
      </section>
    </main>

    <footer class="footer">
      <div class="container footer__grid">
        <div>
          <div class="brand brand--footer">
            <span class="brand__title">SpecDiff</span>
            <span class="brand__subtitle">Project page</span>
          </div>
          <p class="footer__note">
            Built as a lightweight static site for GitHub Pages. Replace the placeholders with your preferred
            affiliations, additional links (code/demo), and figures from the papers.
          </p>
        </div>
        <div class="footer__links" aria-label="Footer links">
          <a href="#top">Top</a>
          <a href="#papers">Papers</a>
          <a href="#blog">Blog</a>
          <a href="#bibtex">BibTeX</a>
        </div>
      </div>
    </footer>

    <script src="assets/js/main.js"></script>
  </body>
</html>
